{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search - Code Sample for chunking documents and generating vector embeddings via indexer composition\n",
    "\n",
    "This code demonstrates a pattern of composing multiple indexers to ingest content from blob storage documents, chunk them, generate embeddings and store them as their own documents in a search index. The code sample here will demonstrate storing the chunked document fragments in their own index, but users can choose to co-locate them in the same index if needed. The following image describes this composition pattern\n",
    "\n",
    "![Indexer chunk composition](../data/images/indexer-composition.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "To run the code, install the following packages. Please use the latest pre-release version `pip install azure-search-documents --pre`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting azure-search-documents\n",
      "  Obtaining dependency information for azure-search-documents from https://files.pythonhosted.org/packages/25/f4/ec7c1d6bafb037d3017db93ef44e18efe84e6d4e7b8906153a9bb777786e/azure_search_documents-11.4.0b8-py3-none-any.whl.metadata\n",
      "  Downloading azure_search_documents-11.4.0b8-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting azure-core<2.0.0,>=1.24.0 (from azure-search-documents)\n",
      "  Obtaining dependency information for azure-core<2.0.0,>=1.24.0 from https://files.pythonhosted.org/packages/98/3a/d53e2b8a75c448ef45d7ae4b0659eb6c0d48978f25a709e2a39894a48704/azure_core-1.29.4-py3-none-any.whl.metadata\n",
      "  Downloading azure_core-1.29.4-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting azure-common~=1.1 (from azure-search-documents)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting isodate>=0.6.0 (from azure-search-documents)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.18.4 (from azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Obtaining dependency information for requests>=2.18.4 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/vscode/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents) (1.16.0)\n",
      "Collecting typing-extensions>=4.6.0 (from azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Obtaining dependency information for typing-extensions>=4.6.0 from https://files.pythonhosted.org/packages/63/d6/ebc4ad51f7a42b0e46c42693563f38b4299314778a773a802a968d6ae742/typing_extensions-4.8.0rc1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.8.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/a4/65/057bf29660aae6ade0816457f8db4e749e5c0bfa2366eb5f67db9912fa4c/charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading azure_search_documents-11.4.0b8-py3-none-any.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.6/305.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.29.4-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0rc1-py3-none-any.whl (31 kB)\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: azure-common, urllib3, typing-extensions, isodate, idna, charset-normalizer, certifi, requests, azure-core, azure-search-documents\n",
      "Successfully installed azure-common-1.1.28 azure-core-1.29.4 azure-search-documents-11.4.0b8 certifi-2023.7.22 charset-normalizer-3.2.0 idna-3.4 isodate-0.6.1 requests-2.31.0 typing-extensions-4.8.0rc1 urllib3-2.0.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/ae/59/911d6e5f1d7514d79c527067643376cddcf4cb8d1728e599b3b03ab51c69/openai-0.28.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /home/vscode/.local/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Collecting tqdm (from openai)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from openai)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/3e/f6/fcda07dd1e72260989f0b22dde999ecfe80daa744f23ca167083683399bc/aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->openai)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, multidict, frozenlist, attrs, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 frozenlist-1.4.0 multidict-6.0.4 openai-0.28.0 tqdm-4.66.1 yarl-1.9.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai[datalib] in /home/vscode/.local/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/vscode/.local/lib/python3.10/site-packages (from openai[datalib]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from openai[datalib]) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /home/vscode/.local/lib/python3.10/site-packages (from openai[datalib]) (3.8.5)\n",
      "Collecting numpy (from openai[datalib])\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas>=1.2.3 (from openai[datalib])\n",
      "  Obtaining dependency information for pandas>=1.2.3 from https://files.pythonhosted.org/packages/fb/4f/4a4372b2e24439f559b73318683486831d75e59544ae02bf8dec8dd6f48b/pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pandas-stubs>=1.1.0.11 (from openai[datalib])\n",
      "  Obtaining dependency information for pandas-stubs>=1.1.0.11 from https://files.pythonhosted.org/packages/0c/e9/e9550b05e262afc75b014b79224b6910f7cbc85cc40448b709ac90a58ecd/pandas_stubs-2.0.3.230814-py3-none-any.whl.metadata\n",
      "  Downloading pandas_stubs-2.0.3.230814-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting openpyxl>=3.0.7 (from openai[datalib])\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile (from openpyxl>=3.0.7->openai[datalib])\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.10/site-packages (from pandas>=1.2.3->openai[datalib]) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2.3->openai[datalib])\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.2.3->openai[datalib])\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting types-pytz>=2022.1.1 (from pandas-stubs>=1.1.0.11->openai[datalib])\n",
      "  Obtaining dependency information for types-pytz>=2022.1.1 from https://files.pythonhosted.org/packages/97/54/936703c579135395210c6819cfaeda02264e024b2a1a11990fd8bf1d5dc2/types_pytz-2023.3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading types_pytz-2023.3.0.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai[datalib]) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai[datalib]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai[datalib]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.20->openai[datalib]) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai[datalib]) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai[datalib]) (1.16.0)\n",
      "Downloading pandas-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas_stubs-2.0.3.230814-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_pytz-2023.3.0.1-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: types-pytz, pytz, tzdata, numpy, et-xmlfile, pandas-stubs, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 numpy-1.25.2 openpyxl-3.1.2 pandas-2.1.0 pandas-stubs-2.0.3.230814 pytz-2023.3.post1 types-pytz-2023.3.0.1 tzdata-2023.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting azure-storage-blob\n",
      "  Obtaining dependency information for azure-storage-blob from https://files.pythonhosted.org/packages/f1/06/68c50a905e1e5481b04a6166b69fecddb87681aae7a556ab727f8e8e6f70/azure_storage_blob-12.17.0-py3-none-any.whl.metadata\n",
      "  Downloading azure_storage_blob-12.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /home/vscode/.local/lib/python3.10/site-packages (from azure-storage-blob) (1.29.4)\n",
      "Collecting cryptography>=2.1.4 (from azure-storage-blob)\n",
      "  Obtaining dependency information for cryptography>=2.1.4 from https://files.pythonhosted.org/packages/46/74/f9eba8c947f57991b5dd5e45797fdc68cc70e444c32e6b952b512d42aba5/cryptography-41.0.3-cp37-abi3-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading cryptography-41.0.3-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from azure-storage-blob) (4.8.0rc1)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/vscode/.local/lib/python3.10/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.18.4 in /home/vscode/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/vscode/.local/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.1.4->azure-storage-blob)\n",
      "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycparser (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "Downloading azure_storage_blob-12.17.0-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-41.0.3-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycparser, cffi, cryptography, azure-storage-blob\n",
      "Successfully installed azure-storage-blob-12.17.0 cffi-1.15.1 cryptography-41.0.3 pycparser-2.21\n"
     ]
    }
   ],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install openai[datalib]\n",
    "! pip install python-dotenv\n",
    "! pip install azure-storage-blob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the custom web API skill for chunking + embedding\n",
    "\n",
    "This sample code also relies on a custom web api skill to be deployed to Azure functions. The custom web api skill performs chunking of content and then generates vector embeddings from the content utilizing Azure Open AI service. The code for the custom web api skill is available as an [Azure Cognitive Search power skill](https://github.com/Azure-Samples/azure-search-power-skills/blob/main/Vector/EmbeddingGenerator/README.md) and can be easily deployed via Visual Studio Code to Azure functions.\n",
    "\n",
    "Please follow those steps first and ensure that the function app is running before proceeding with the rest of the sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring storage accounts for deletion detection\n",
    "\n",
    "The storage accounts used for storing both the source documents as well as for storing the knowledge store projections (after the \"chunking + embedding\" step) need to adhere to the following requirements, in order to seamlessly track document deletes:\n",
    "\n",
    "1. They need to be of type \"Standard general-purpose v2\".\n",
    "2. They need to have soft delete enabled. Learn more [here](https://learn.microsoft.com/azure/storage/blobs/soft-delete-blob-enable?tabs=azure-portal)\n",
    "\n",
    "Learn more about deletion detection policies used in Azure Cognitive Search [here](https://learn.microsoft.com/azure/search/search-howto-index-changed-deleted-blobs?tabs=portal#native-blob-soft-delete-preview)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage the index, data source, skillset and indexer for the source document\n",
    "\n",
    "The following code will configure an index to hold the source documents, via an indexer that reads data from an Azure storage container that is able to generate embeddings and write that to a separate storage account (knowledge store)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    FieldMapping,\n",
    "    FieldMappingFunction,\n",
    "    InputFieldMappingEntry, \n",
    "    OutputFieldMappingEntry, \n",
    "    SearchIndexerSkillset,\n",
    "    SearchIndexerKnowledgeStore,\n",
    "    SearchIndexerKnowledgeStoreProjection,\n",
    "    SearchIndexerKnowledgeStoreFileProjectionSelector,\n",
    "    IndexingParameters, \n",
    "    WebApiSkill,\n",
    "    SearchIndex,\n",
    "    SemanticSettings,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING = os.getenv(\"AZURE_KNOWLEDGE_STORE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "def get_index_client() -> SearchIndexClient:\n",
    "    return SearchIndexClient(AZURE_SEARCH_SERVICE_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_KEY))\n",
    "\n",
    "def get_indexer_client() -> SearchIndexerClient:\n",
    "    return SearchIndexerClient(AZURE_SEARCH_SERVICE_ENDPOINT, AzureKeyCredential(AZURE_SEARCH_KEY))\n",
    "\n",
    "def get_index_name(index_prefix):\n",
    "    return f\"{index_prefix}-index\"\n",
    "\n",
    "def get_datasource_name(index_prefix):\n",
    "    return f\"{index_prefix}-datasource\"\n",
    "\n",
    "def get_skillset_name(index_prefix):\n",
    "    return f\"{index_prefix}-skillset\"\n",
    "\n",
    "def get_indexer_name(index_prefix):\n",
    "    return f\"{index_prefix}-indexer\"\n",
    "\n",
    "def get_chunk_index_blob_container_name(index_prefix):\n",
    "    return f\"{index_prefix}ChunkIndex\".replace('-', '').lower()\n",
    "\n",
    "def get_knowledge_store_connection_string():\n",
    "    return AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define simple utilities to to help configure index, data source, skillset (with knowledge store) and indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(index_name, fields, vector_search, semantic_title_field_name, semantic_content_field_names):\n",
    "    semantic_settings = SemanticSettings(\n",
    "        configurations=[SemanticConfiguration(\n",
    "            name='default',\n",
    "            prioritized_fields=PrioritizedFields(\n",
    "                title_field=SemanticField(field_name=semantic_title_field_name), prioritized_content_fields=[SemanticField(field_name=field_name) for field_name in semantic_content_field_names]))])\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_settings=semantic_settings)\n",
    "    index_client = get_index_client()\n",
    "    return index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blob_datasource(datasource_name, storage_connection_string, container_name):\n",
    "    # This example utilizes a REST request as the python SDK doesn't support the blob soft delete policy yet\n",
    "    api_version = '2023-07-01-Preview'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': f'{AZURE_SEARCH_KEY}'\n",
    "    }\n",
    "    data_source = {\n",
    "        \"name\": datasource_name,\n",
    "        \"type\": \"azureblob\",\n",
    "        \"credentials\": {\"connectionString\": storage_connection_string},\n",
    "        \"container\": {\"name\": container_name},\n",
    "        \"dataDeletionDetectionPolicy\": {\"@odata.type\": \"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\"}\n",
    "    }\n",
    "\n",
    "    url = '{}/datasources/{}?api-version={}'.format(AZURE_SEARCH_SERVICE_ENDPOINT, datasource_name, api_version)\n",
    "    response = requests.put(url, json=data_source, headers=headers)\n",
    "\n",
    "    ds_client = get_indexer_client()\n",
    "    return ds_client.get_data_source_connection(datasource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_indexer_completion(indexer_name):\n",
    "    indexer_client = get_indexer_client()\n",
    "    # poll status and wait until indexer is complete\n",
    "    status = f\"Indexer {indexer_name} not started yet\"\n",
    "    while (indexer_client.get_indexer_status(indexer_name).last_result == None) or ((status := indexer_client.get_indexer_status(indexer_name).last_result.status) != \"success\"):\n",
    "        print(f\"Indexing status:{status}\")\n",
    "\n",
    "        # It's possible that the indexer may reach a state of transient failure, especially when generating embeddings\n",
    "        # via Open AI. For the purposes of the demo, we'll just break out of the loop and continue with the rest of the steps.\n",
    "        if (status == \"transientFailure\"):\n",
    "            print(f\"Indexer {indexer_name} failed before fully indexing documents\")\n",
    "            break\n",
    "        time.sleep(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to manage the \"source\" document index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIndexManager():\n",
    "    def _create_document_index(self, index_prefix):\n",
    "        name = get_index_name(index_prefix)\n",
    "        fields = [\n",
    "            SimpleField(name=\"document_id\", type=SearchFieldDataType.String, filterable=True, sortable=True, key=True),\n",
    "            SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"filesize\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"filepath\", type=SearchFieldDataType.String)\n",
    "        ]\n",
    "        return create_index(name, fields, vector_search=None, semantic_title_field_name=\"filepath\", semantic_content_field_names=[\"content\"])\n",
    "\n",
    "    def _create_document_datasource(self, index_prefix, storage_connection_string, container_name):\n",
    "        name = get_datasource_name(index_prefix)\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name)\n",
    "\n",
    "    def _create_document_skillset(self, index_prefix, content_field_name=\"content\"):\n",
    "        embedding_skill_endpoint = os.getenv(\"AZURE_SEARCH_EMBEDDING_SKILL_ENDPOINT\")\n",
    "\n",
    "        name = get_skillset_name(index_prefix)\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "        content_context = f\"/document/{content_field_name}\"\n",
    "        embedding_skill = WebApiSkill(\n",
    "                            name=\"chunking-embedding-skill\",\n",
    "                            uri=embedding_skill_endpoint,\n",
    "                            timeout=\"PT3M\",\n",
    "                            batch_size=1,\n",
    "                            degree_of_parallelism=1,\n",
    "                            context=content_context,\n",
    "                            inputs=[\n",
    "                                    InputFieldMappingEntry(name=\"document_id\", source=\"/document/document_id\"),\n",
    "                                    InputFieldMappingEntry(name=\"text\", source=content_context),\n",
    "                                    InputFieldMappingEntry(name=\"filepath\", source=\"/document/filepath\"),\n",
    "                                    InputFieldMappingEntry(name=\"fieldname\", source=f\"='{content_field_name}'\")],\n",
    "                            outputs=[OutputFieldMappingEntry(name=\"chunks\", target_name=\"chunks\")])\n",
    "        knowledge_store = SearchIndexerKnowledgeStore(storage_connection_string=get_knowledge_store_connection_string(),\n",
    "                                                    projections=[\n",
    "                                                                SearchIndexerKnowledgeStoreProjection(\n",
    "                                                                    objects=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                                                                        storage_container=chunk_index_blob_container_name,\n",
    "                                                                        generated_key_name=\"id\",\n",
    "                                                                        source_context=f\"{content_context}/chunks/*\",\n",
    "                                                                        inputs=[\n",
    "                                                                            InputFieldMappingEntry(name=\"source_document_id\", source=\"/document/document_id\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"source_document_filepath\", source=\"/document/filepath\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"source_field_name\", source=f\"{content_context}/chunks/*/embedding_metadata/fieldname\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"title\", source=f\"{content_context}/chunks/*/title\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"text\", source=f\"{content_context}/chunks/*/content\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"embedding\", source=f\"{content_context}/chunks/*/embedding_metadata/embedding\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"index\", source=f\"{content_context}/chunks/*/embedding_metadata/index\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"offset\", source=f\"{content_context}/chunks/*/embedding_metadata/offset\"),\n",
    "                                                                            InputFieldMappingEntry(name=\"length\", source=f\"{content_context}/chunks/*/embedding_metadata/length\")                                                                            \n",
    "                                                                            ]\n",
    "                                                                            )\n",
    "                                                                    ]),\n",
    "                                                                SearchIndexerKnowledgeStoreProjection(\n",
    "                                                                files=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                                                                    storage_container=f\"{chunk_index_blob_container_name}images\",\n",
    "                                                                    generated_key_name=\"imagepath\",\n",
    "                                                                    source=\"/document/normalized_images/*\",\n",
    "                                                                    inputs=[]\n",
    "                                                                        )\n",
    "                                                                ])\n",
    "                                                                ])\n",
    "        skillset = SearchIndexerSkillset(name=name, skills=[embedding_skill], description=name, knowledge_store=knowledge_store)\n",
    "        client = get_indexer_client()\n",
    "        return client.create_skillset(skillset)\n",
    "\n",
    "    def _create_document_indexer(self, index_prefix, data_source_name, index_name, skillset_name, content_field_name=\"content\", generate_page_images=True):\n",
    "        content_context = f\"/document/{content_field_name}\"\n",
    "        name = get_indexer_name(index_prefix)\n",
    "        indexer_config = {\"dataToExtract\": \"contentAndMetadata\", \"imageAction\": \"generateNormalizedImagePerPage\"} if generate_page_images else {\"dataToExtract\": \"contentAndMetadata\"}\n",
    "        parameters = IndexingParameters(max_failed_items = -1, configuration=indexer_config)\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            skillset_name=skillset_name,\n",
    "            field_mappings=[FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"document_id\", mapping_function=FieldMappingFunction(name=\"base64Encode\", parameters=None)),\n",
    "                            FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"filepath\"),\n",
    "                            FieldMapping(source_field_name=\"metadata_storage_size\", target_field_name=\"filesize\")],\n",
    "            output_field_mappings=[],\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client()\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "    def create_document_index_resources(self, index_prefix, customer_storage_connection_string, customer_container_name) -> dict:\n",
    "        index_name = self._create_document_index(index_prefix).name\n",
    "        data_source_name = self._create_document_datasource(index_prefix, customer_storage_connection_string, customer_container_name).name\n",
    "        skillset_name = self._create_document_skillset(index_prefix).name    \n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_document_indexer(index_prefix, data_source_name, index_name, skillset_name).name\n",
    "        wait_for_indexer_completion(indexer_name)\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"skillset_name\": skillset_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "    def delete_document_index_resources(self, index_prefix):\n",
    "        index_client = get_index_client()\n",
    "        indexer_client = get_indexer_client()\n",
    "\n",
    "        index_client.delete_index(index=get_index_name(index_prefix))\n",
    "        indexer_client.delete_indexer(indexer=get_indexer_name(index_prefix))\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=get_datasource_name(index_prefix))\n",
    "        indexer_client.delete_skillset(skillset=get_skillset_name(index_prefix))\n",
    "\n",
    "        # delete the knowledge store tables and blobs\n",
    "        knowledge_store_connection_string  = get_knowledge_store_connection_string()\n",
    "        \n",
    "        # delete the container directly from storage\n",
    "        try:\n",
    "            blob_service = BlobServiceClient.from_connection_string(knowledge_store_connection_string)\n",
    "            blob_service.delete_container(get_chunk_index_blob_container_name(index_prefix))\n",
    "        # handle resource not found error\n",
    "        except ResourceNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to manage the \"chunked\" document index - with vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkIndexManager():\n",
    "\n",
    "    def _create_chunk_index(self, index_prefix):\n",
    "        name = get_index_name(f\"{index_prefix}-chunk\")\n",
    "        vector_search = VectorSearch(\n",
    "            algorithm_configurations=[\n",
    "                HnswVectorSearchAlgorithmConfiguration(\n",
    "                    name=\"my-vector-config\",\n",
    "                    kind=\"hnsw\",\n",
    "                    parameters={\n",
    "                        \"m\": 4,\n",
    "                        \"efConstruction\": 400,\n",
    "                        \"efSearch\": 1000,\n",
    "                        \"metric\": \"cosine\"\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, facetable=True, filterable=True, sortable=True, key=True),            \n",
    "            SimpleField(name=\"source_document_id\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_document_filepath\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_field_name\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"title\", type=SearchFieldDataType.String),   \n",
    "            SimpleField(name=\"index\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"offset\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"length\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"hash\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"text\", type=SearchFieldDataType.String),                 \n",
    "            SearchField(name=\"embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"my-vector-config\")    \n",
    "        ]\n",
    "        index = create_index(name, fields, vector_search=vector_search, semantic_title_field_name=\"title\", semantic_content_field_names=[\"text\"])\n",
    "        return index\n",
    "    \n",
    "    def _create_chunk_datasource(self, index_prefix, storage_connection_string, container_name):\n",
    "        name = get_datasource_name(f\"{index_prefix}-chunk\")\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name)\n",
    "\n",
    "    def _create_chunk_indexer(self, index_prefix, data_source_name, index_name):\n",
    "        name = get_indexer_name(f\"{index_prefix}-chunk\")\n",
    "        parameters = IndexingParameters(configuration={\"parsing_mode\": \"json\"})\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client()\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "\n",
    "    def create_chunk_index_resources(self, index_prefix) -> dict:\n",
    "        chunk_index_storage_connection_string = get_knowledge_store_connection_string()\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "\n",
    "        index_name = self._create_chunk_index(index_prefix).name\n",
    "        data_source_name = self._create_chunk_datasource(index_prefix, chunk_index_storage_connection_string, chunk_index_blob_container_name).name\n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_chunk_indexer(index_prefix, data_source_name, index_name).name\n",
    "        wait_for_indexer_completion(indexer_name)\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "\n",
    "    # delete all the resources\n",
    "    def delete_chunk_index_resources(self, index_prefix):\n",
    "        index_client = get_index_client()\n",
    "        indexer_client = get_indexer_client()\n",
    "\n",
    "        index_client.delete_index(index=f\"{index_prefix}-chunk-index\")\n",
    "        indexer_client.delete_indexer(indexer=f\"{index_prefix}-chunk-indexer\")\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=f\"{index_prefix}-chunk-datasource\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embedder utility to aid during query time\n",
    "\n",
    "**NOTE**: Make sure to utilize the same Azure OpenAI Embedding Deployment at query time as the one used in the custom web api skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedder():\n",
    "    openai.api_type = \"azure\"    \n",
    "    openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    openai.api_base = f\"https://{os.getenv('AZURE_OPENAI_SERVICE_NAME')}.openai.azure.com/\"\n",
    "    openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "\n",
    "    def clean_text(self, text, text_limit=7000):\n",
    "        # Clean up text (e.g. line breaks, )    \n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[\\n\\r]+', ' ', text).strip()\n",
    "        # Truncate text if necessary (e.g. for, ada-002, 4095 tokens ~ 7000 chracters)    \n",
    "        if len(text) > text_limit:\n",
    "            logging.warning(\"Token limit reached exceeded maximum length, truncating...\")\n",
    "            text = text[:text_limit]\n",
    "        return text\n",
    "\n",
    "    # Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "    def generate_embeddings(self, text, clean_text=True):\n",
    "        if clean_text:\n",
    "            text = self.clean_text(text)\n",
    "        response = openai.Embedding.create(input=text, engine=self.AZURE_OPENAI_EMBEDDING_DEPLOYMENT)\n",
    "        embeddings = response['data'][0]['embedding']\n",
    "        return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wire up the utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexes(prefix, customer_storage_connection_string, container_name):\n",
    "    index_manager = DocumentIndexManager()\n",
    "    doc_index_resources = index_manager.create_document_index_resources(prefix, customer_storage_connection_string, container_name)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    chunk_index_manager = ChunkIndexManager()\n",
    "    chunk_index_resources = chunk_index_manager.create_chunk_index_resources(prefix)\n",
    "    return {\"doc_index_resources\": doc_index_resources, \"chunk_index_resources\": chunk_index_resources}\n",
    "\n",
    "def delete_indexes(prefix):\n",
    "    index_manager = DocumentIndexManager()\n",
    "    index_manager.delete_document_index_resources(prefix)\n",
    "    chunk_index_manager = ChunkIndexManager()\n",
    "    chunk_index_manager.delete_chunk_index_resources(prefix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The following code will upload a bunch of sample PDFs to the \"source document\" storage account, in the container specified. And will implement the indexer composition pattern to ingest both the content from the source documents as well as the chunked + embedded content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the sample data to blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant ='pythonsample'\n",
    "\n",
    "customer_storage_connection_string = os.getenv(\"DOCUMENT_AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"DOCUMENT_AZURE_STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "prefix = f\"{tenant}-{container_name}\"\n",
    "\n",
    "# Delete any existing Azure Cognitive Search resources\n",
    "delete_indexes(prefix)\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(customer_storage_connection_string)\n",
    "container_client = blob_service_client.get_container_client(container=container_name)\n",
    "\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "# Upload sample documents to blob storage\n",
    "for root, dirs, files in os.walk(\"./data/documents/\"):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), \"rb\") as data:\n",
    "            container_client.upload_blob(file, data, overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Azure Cognitive Search resources\n",
    "\n",
    "**NOTE**: The following example creates the source document indexer and the chunk document indexer, but we wait for the first indexer to fully finish its run before creating the second - this is reasonable with very small amounts of data, but wouldn't scale well for larger data. In that scenario it would make more sense to create the indexers in parallel with a schedule and let them run on their own and converge eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subtype value #Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy has no mapping, use base class DataDeletionDetectionPolicy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing status:Indexer pythonsample-documents-indexer not started yet\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n",
      "Indexing status:inProgress\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subtype value #Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy has no mapping, use base class DataDeletionDetectionPolicy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing status:Indexer pythonsample-documents-chunk-indexer not started yet\n",
      "Indexing status:Indexer pythonsample-documents-chunk-indexer not started yet\n",
      "Indexing status:inProgress\n"
     ]
    }
   ],
   "source": [
    "# ensure indexes\n",
    "index_resources = create_indexes(prefix, customer_storage_connection_string, container_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the \"chunk\" search index with different kinds of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_index(index_name, query, vector_only=False):  \n",
    "    embedder = TextEmbedder()  \n",
    "    vector = Vector(value=embedder.generate_embeddings(query), k=3, fields=\"embedding\")  \n",
    "    search_client = SearchClient(AZURE_SEARCH_SERVICE_ENDPOINT, index_name, AzureKeyCredential(AZURE_SEARCH_KEY))  \n",
    "    if vector_only:  \n",
    "        search_text = None  \n",
    "    else:  \n",
    "        search_text = query  \n",
    "    results = search_client.search(search_text=search_text, vectors=[vector], top=3)  \n",
    "    return results  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector only query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benefit_Options.pdf\n",
      "Content: ntoso Electronics:\n",
      "\n",
      "Next Steps \n",
      "We hope that this information has been helpful in understanding the differences between Northwind \n",
      "Health Plus and Northwind Standard. We are confident that you will find the right plan for you and \n",
      "your family. Thank you for choosing Contoso Electronics! \n",
      "\n",
      "pageimage3.jpg\n",
      "Source Document: Benefit_Options.pdf\n",
      "Title: employee_handbook.pdf\n",
      "Content: nd Protection \n",
      "\n",
      " \n",
      "\n",
      "Contoso Electronics is committed to protecting the security of your personal information. \n",
      "\n",
      "We have implemented physical, technical, and administrative measures to protect your data \n",
      "\n",
      "from unauthorized access, alteration, or disclosure. \n",
      "\n",
      " \n",
      "\n",
      "We use secure servers and encryption technology to protect data transmitted over the \n",
      "\n",
      "Internet. \n",
      "\n",
      " \n",
      "\n",
      "Access to Personal Information \n",
      "\n",
      " \n",
      "\n",
      "You have the right to access, review, and request a copy of your personal information that \n",
      "\n",
      "we have collected and stored. You may also request that we delete or correct any inaccurate \n",
      "\n",
      "information. \n",
      "\n",
      " \n",
      "\n",
      "To access or make changes to your personal information, please contact the Privacy Officer \n",
      "\n",
      "at privacy@contoso.com. \n",
      "\n",
      " \n",
      "\n",
      "Changes to This Policy \n",
      "\n",
      " \n",
      "\n",
      "We may update this policy from time to time to reflect changes in our practices or \n",
      "\n",
      "applicable laws. We will notify you of any changes by posting a revised policy on our \n",
      "\n",
      "pageimage6.jpg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "website. \n",
      "\n",
      " \n",
      "\n",
      "Questions or Concerns \n",
      "\n",
      " \n",
      "\n",
      "If you have any questions or concerns about our privacy policies or practices, please contact \n",
      "\n",
      "the Privacy Officer at privacy@contoso.com. \n",
      "\n",
      "Whistleblower Policy \n",
      " \n",
      "\n",
      "Contoso Electronics Whistleblower Policy \n",
      "\n",
      " \n",
      "\n",
      "At Contoso Electronics, we believe in maintaining a safe and transparent working \n",
      "\n",
      "environment for all of our team members. To ensure the well-being of the entire \n",
      "\n",
      "organization, we have established a Whistleblower Policy. This policy encourages \n",
      "\n",
      "employees to come forth and report any unethical or illegal activities they may witness \n",
      "\n",
      "while working at Contoso Electronics. \n",
      "\n",
      " \n",
      "\n",
      "This policy applies to all Contoso Electronics employees, contractors, and other third \n",
      "\n",
      "parties. \n",
      "\n",
      " \n",
      "\n",
      "Definition: \n",
      "\n",
      " \n",
      "\n",
      "A whistleblower is an individual who reports activities that are illegal, unethical, or \n",
      "\n",
      "otherwise not in accordance with company policy. \n",
      "\n",
      " \n",
      "\n",
      "Reporting Procedures: \n",
      "\n",
      " \n",
      "\n",
      "If you witness any activity that you believe to be illegal, unethical, or not in accordance with \n",
      "\n",
      "company policy, it is important that you report it immediately. You can do this by: \n",
      "\n",
      " \n",
      "\n",
      "1. Contacting the Human Resources Depart\n",
      "Source Document: employee_handbook.pdf\n",
      "Title: Northwind_Standard_Benefits_Details.pdf\n",
      "Content: rescription medications for allergies, such as antihistamines and decongestants \n",
      "\n",
      "• Allergy medications for asthma, such as albuterol \n",
      "\n",
      "• Allergy medications for skin conditions, such as topical corticosteroids \n",
      "\n",
      "• Nasal sprays for allergies \n",
      "\n",
      "Exceptions \n",
      "\n",
      "The Northwind Standard plan does not cover the cost of allergy testing or treatment for \n",
      "\n",
      "cosmetic purposes. \n",
      "\n",
      "Tips \n",
      "\n",
      "• Make sure to tell your doctor about any medications you are currently taking, as some \n",
      "\n",
      "medications can interfere with allergy testing or treatment. \n",
      "\n",
      "• If you are prescribed medications for allergies, be sure to follow your doctor’s instructions \n",
      "\n",
      "carefully. \n",
      "\n",
      "• Ask your doctor about other treatments that may be available, such as immunotherapy \n",
      "\n",
      "(allergy shots). \n",
      "\n",
      "• If you are prescribed medications for allergies, be sure to check with your pharmacist to \n",
      "\n",
      "see if there are any generic or over-the-counter options that may be more affordable. \n",
      "\n",
      "pageimage18.jpg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "• Keep track of your allergies, including the type of allergy, the severity of the allergy, the \n",
      "\n",
      "medications you are taking, and the results of any allergy tests you have had. This \n",
      "\n",
      "information can be helpful for your doctor when making decisions about your care. \n",
      "\n",
      "Ambulance \n",
      "\n",
      "COVERED SERVICES: Ambulance  \n",
      "\n",
      "Ambulance services are covered under the Northwind Standard plan, providing you with \n",
      "\n",
      "the medical assistance you need in the event of an emergency. When you are in need of an \n",
      "\n",
      "ambulance, you can be sure that Northwind Health will cover your transport to the closest \n",
      "\n",
      "hospital or medical facility. \n",
      "\n",
      "Covered Services:  \n",
      "\n",
      "The Northwind Standard plan covers ambulance transport to the nearest hospital or \n",
      "\n",
      "medical facility in the event of an emergency. This service is available 24 hours a day, seven \n",
      "\n",
      "days a week, and is covered up to the plan's limit. Ambulance transport is covered up to the \n",
      "\n",
      "plan's limit, regardless of whether the ambulance is provided by an in-network provider or \n",
      "\n",
      "an out-of-network provider.  \n",
      "\n",
      "Exceptions:  \n",
      "\n",
      "Northwind Health does not cover ambulance services that are provided for non-emergen\n",
      "Source Document: Northwind_Standard_Benefits_Details.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_index_name = index_resources[\"chunk_index_resources\"][\"index_name\"]  \n",
    "results = query_vector_index(chunk_index_name, \"hearing aid\", vector_only=True)  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Content: {result['text']}\")  \n",
    "    print(f\"Source Document: {result['source_document_filepath']}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benefit_Options.pdf\n",
      "Content: ntoso Electronics:\n",
      "\n",
      "Next Steps \n",
      "We hope that this information has been helpful in understanding the differences between Northwind \n",
      "Health Plus and Northwind Standard. We are confident that you will find the right plan for you and \n",
      "your family. Thank you for choosing Contoso Electronics! \n",
      "\n",
      "pageimage3.jpg\n",
      "Source Document: Benefit_Options.pdf\n",
      "Title: employee_handbook.pdf\n",
      "Content: nd Protection \n",
      "\n",
      " \n",
      "\n",
      "Contoso Electronics is committed to protecting the security of your personal information. \n",
      "\n",
      "We have implemented physical, technical, and administrative measures to protect your data \n",
      "\n",
      "from unauthorized access, alteration, or disclosure. \n",
      "\n",
      " \n",
      "\n",
      "We use secure servers and encryption technology to protect data transmitted over the \n",
      "\n",
      "Internet. \n",
      "\n",
      " \n",
      "\n",
      "Access to Personal Information \n",
      "\n",
      " \n",
      "\n",
      "You have the right to access, review, and request a copy of your personal information that \n",
      "\n",
      "we have collected and stored. You may also request that we delete or correct any inaccurate \n",
      "\n",
      "information. \n",
      "\n",
      " \n",
      "\n",
      "To access or make changes to your personal information, please contact the Privacy Officer \n",
      "\n",
      "at privacy@contoso.com. \n",
      "\n",
      " \n",
      "\n",
      "Changes to This Policy \n",
      "\n",
      " \n",
      "\n",
      "We may update this policy from time to time to reflect changes in our practices or \n",
      "\n",
      "applicable laws. We will notify you of any changes by posting a revised policy on our \n",
      "\n",
      "pageimage6.jpg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "website. \n",
      "\n",
      " \n",
      "\n",
      "Questions or Concerns \n",
      "\n",
      " \n",
      "\n",
      "If you have any questions or concerns about our privacy policies or practices, please contact \n",
      "\n",
      "the Privacy Officer at privacy@contoso.com. \n",
      "\n",
      "Whistleblower Policy \n",
      " \n",
      "\n",
      "Contoso Electronics Whistleblower Policy \n",
      "\n",
      " \n",
      "\n",
      "At Contoso Electronics, we believe in maintaining a safe and transparent working \n",
      "\n",
      "environment for all of our team members. To ensure the well-being of the entire \n",
      "\n",
      "organization, we have established a Whistleblower Policy. This policy encourages \n",
      "\n",
      "employees to come forth and report any unethical or illegal activities they may witness \n",
      "\n",
      "while working at Contoso Electronics. \n",
      "\n",
      " \n",
      "\n",
      "This policy applies to all Contoso Electronics employees, contractors, and other third \n",
      "\n",
      "parties. \n",
      "\n",
      " \n",
      "\n",
      "Definition: \n",
      "\n",
      " \n",
      "\n",
      "A whistleblower is an individual who reports activities that are illegal, unethical, or \n",
      "\n",
      "otherwise not in accordance with company policy. \n",
      "\n",
      " \n",
      "\n",
      "Reporting Procedures: \n",
      "\n",
      " \n",
      "\n",
      "If you witness any activity that you believe to be illegal, unethical, or not in accordance with \n",
      "\n",
      "company policy, it is important that you report it immediately. You can do this by: \n",
      "\n",
      " \n",
      "\n",
      "1. Contacting the Human Resources Depart\n",
      "Source Document: employee_handbook.pdf\n",
      "Title: Northwind_Standard_Benefits_Details.pdf\n",
      "Content: rescription medications for allergies, such as antihistamines and decongestants \n",
      "\n",
      "• Allergy medications for asthma, such as albuterol \n",
      "\n",
      "• Allergy medications for skin conditions, such as topical corticosteroids \n",
      "\n",
      "• Nasal sprays for allergies \n",
      "\n",
      "Exceptions \n",
      "\n",
      "The Northwind Standard plan does not cover the cost of allergy testing or treatment for \n",
      "\n",
      "cosmetic purposes. \n",
      "\n",
      "Tips \n",
      "\n",
      "• Make sure to tell your doctor about any medications you are currently taking, as some \n",
      "\n",
      "medications can interfere with allergy testing or treatment. \n",
      "\n",
      "• If you are prescribed medications for allergies, be sure to follow your doctor’s instructions \n",
      "\n",
      "carefully. \n",
      "\n",
      "• Ask your doctor about other treatments that may be available, such as immunotherapy \n",
      "\n",
      "(allergy shots). \n",
      "\n",
      "• If you are prescribed medications for allergies, be sure to check with your pharmacist to \n",
      "\n",
      "see if there are any generic or over-the-counter options that may be more affordable. \n",
      "\n",
      "pageimage18.jpg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "• Keep track of your allergies, including the type of allergy, the severity of the allergy, the \n",
      "\n",
      "medications you are taking, and the results of any allergy tests you have had. This \n",
      "\n",
      "information can be helpful for your doctor when making decisions about your care. \n",
      "\n",
      "Ambulance \n",
      "\n",
      "COVERED SERVICES: Ambulance  \n",
      "\n",
      "Ambulance services are covered under the Northwind Standard plan, providing you with \n",
      "\n",
      "the medical assistance you need in the event of an emergency. When you are in need of an \n",
      "\n",
      "ambulance, you can be sure that Northwind Health will cover your transport to the closest \n",
      "\n",
      "hospital or medical facility. \n",
      "\n",
      "Covered Services:  \n",
      "\n",
      "The Northwind Standard plan covers ambulance transport to the nearest hospital or \n",
      "\n",
      "medical facility in the event of an emergency. This service is available 24 hours a day, seven \n",
      "\n",
      "days a week, and is covered up to the plan's limit. Ambulance transport is covered up to the \n",
      "\n",
      "plan's limit, regardless of whether the ambulance is provided by an in-network provider or \n",
      "\n",
      "an out-of-network provider.  \n",
      "\n",
      "Exceptions:  \n",
      "\n",
      "Northwind Health does not cover ambulance services that are provided for non-emergen\n",
      "Source Document: Northwind_Standard_Benefits_Details.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_index_name = index_resources[\"chunk_index_resources\"][\"index_name\"]\n",
    "results = query_vector_index(chunk_index_name, \"hearing aid\")\n",
    "for result in results:\n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Content: {result['text']}\")  \n",
    "    print(f\"Source Document: {result['source_document_filepath']}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
